{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Notes\n",
    "\n",
    "Utilizing this notebook requires you to download the new hdf5 formatted data files. These will *eventually* be found in the same directory as the previous data sets.\n",
    "\n",
    "Once again, it should be noted that this should not be run in notebook form, but rather from within a monitoring script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import h5py\n",
    "from keras.layers import Convolution2D, MaxPooling2D # Layer Def\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\mu$BooNEDataGenerator for Large Data Sets\n",
    "\n",
    "This was created in all of about 5 minutes. Therefore, don't expect it to be robust at all.\n",
    "The idea here is that since the data sets are rather large, the generator only pulls out a slice at a time.\n",
    "\n",
    "For the HDF5 data, the frames have been compressed using `gzip` and `chunk`'d with a chunk size of `10`.\n",
    "\n",
    "Therefore, for the sake of performance, the batch size I'm definining in the generator is the same as the chunk size.\n",
    "\n",
    "Lastly, I tend to prefer class oriented generators over functional models as this will be allow me later to check the state of the generator after training completes. This check I just mentioned has been omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UBooNEDataGenerator(object):\n",
    "  logger = logging.getLogger(\"uboone.data\")\n",
    "  def __init__(self, datapath, dataset, labelset):\n",
    "    self.logger.info(\"Assembling DataSet\")\n",
    "    self._file = h5py.File(datapath,'r')\n",
    "    self._dataset = self._file[dataset]\n",
    "    self._labelset = self._file[labelset]\n",
    "    self.current_index=0\n",
    "\n",
    "  def __len__(self):\n",
    "    return self._dataset.shape[0]\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    return self.next()\n",
    "\n",
    "  def next(self):\n",
    "    batch_size = 10\n",
    "    #This next bit causes the generator to loop indefinitely\n",
    "    if self.current_index>= len(self):\n",
    "        self.logger.info(\"Reusing Data at Size: {}\".format(len(self)))\n",
    "        self.current_index = 0\n",
    "    if self.current_index+batch_size>= len(self):\n",
    "        batch_size = len(self)-current_index\n",
    "    x = self._dataset[self.current_index:self.current_index+batch_size]\n",
    "    y = self._labelset[self.current_index:self.current_index+batch_size]\n",
    "    self.current_index+=batch_size\n",
    "\n",
    "    return (x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "Once again, for the sake of performance, I've pre-defined out the network parameters. The indexing of the data sets here is critical as this defines the output of each successive layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG16(Model):\n",
    "  logger = logging.getLogger('uboone.vgg16')\n",
    "  def __init__(self):\n",
    "\n",
    "    self.logger.info(\"Assembling Model\")\n",
    "    # The input shape is defined as 3 planes at 576x576 pixels\n",
    "    # TODO: I think with the Theano backend, this might need to be reversed.\n",
    "\n",
    "    if K.image_dim_ordering() != 'th':\n",
    "        self.logger.error(\"Dimension Ordering Incorrect\")\n",
    "\n",
    "    self._input = Input(shape=(3,576,576))\n",
    "    #self.logger.debug(\"Input Shape: {}\".format(self._input.output_shape))\n",
    "\n",
    "    # Block 1\n",
    "    layer = Convolution2D(64, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block1_conv1')(self._input)\n",
    "    layer = Convolution2D(64, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block1_conv2')(layer)\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(layer)\n",
    "\n",
    "    # Block 2\n",
    "    layer = Convolution2D(128, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block2_conv1')(layer)\n",
    "    layer = Convolution2D(128, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block2_conv2')(layer)\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(layer)\n",
    "\n",
    "    # Block 3\n",
    "    layer = Convolution2D(256, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block3_conv1')(layer)\n",
    "    layer = Convolution2D(256, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block3_conv2')(layer)\n",
    "    layer = Convolution2D(256, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block3_conv3')(layer)\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(layer)\n",
    "\n",
    "    # Block 4\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block4_conv1')(layer)\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block4_conv2')(layer)\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block4_conv3')(layer)\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(layer)\n",
    "\n",
    "    # Block 5\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block5_conv1')(layer)\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block5_conv2')(layer)\n",
    "    layer = Convolution2D(512, 3, 3, activation='relu', border_mode='same', \n",
    "                          name='block5_conv3')(layer)\n",
    "    layer = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(layer)\n",
    "\n",
    "    # Classification block\n",
    "    layer = Flatten(name='flatten')(layer)\n",
    "    layer = Dense(4096, activation='relu', name='fc1')(layer)\n",
    "    layer = Dense(4096, activation='relu', name='fc2')(layer)\n",
    "    layer = Dense(5, activation='softmax', name='predictions')(layer)\n",
    "    \n",
    "    super(VGG16, self).__init__(self._input, layer)\n",
    "    self.logger.info(\"Compiling Model\")\n",
    "    self.compile(loss='binary_crossentropy', optimizer='sgd')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training step\n",
    "\n",
    "First of all, I'm a huge fan of using the logging module as this will allow me to monitor performance externally. Thus, the first steps are to setup the logging configuration.\n",
    "\n",
    "The data generator is the next thing to be instantiated. Here, the network is setup to train on the input tpc images with an output expected to match particle type.\n",
    "\n",
    "Lastly, the model is instantiated and call to `train` or `fit` as those terms seem to be interchangeable in Keras.\n",
    "The samples per epoch and number of epoch figures were chosen arbitrarily. This was done out of sheer laziness and due to the fact that this was written at the same time as the data was being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.info(\"Starting...\")\n",
    "\n",
    "  data_generator = UBooNEDataGenerator('hdf5/eminus_train.h5', 'image2d/tpc', 'labels/type')\n",
    "\n",
    "  model = VGG16()\n",
    "  model.fit_generator(data_generator, samples_per_epoch = 20000, nb_epoch=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
